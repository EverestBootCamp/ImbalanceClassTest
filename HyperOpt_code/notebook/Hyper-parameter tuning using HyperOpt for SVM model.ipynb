{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Goal :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform hyper-parameter tuning using HyperOpt for SVM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning with hyperopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning is an important step for maximizing the performance of a model. Hyperparameters are certain values/weights that determine the learning process of an algorithm. Several Python packages have been developed specifically for this purpose. Scikit-learn provides a few options, GridSearchCV and RandomizedSearchCV being two of the more popular options. Outside of scikit-learn, the Optunity, Spearmint and hyperopt packages are all designed for optimization. In this task, we will focus on the hyperopt package.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperOpt:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a powerful python library that search through an hyperparameter space of values . It implements three functions for minimizing the cost function,\n",
    "\n",
    "* Random Search\n",
    "* TPE (Tree Parzen Estimators)\n",
    "* Adaptive TPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "data_transformed = pd.read_csv('data_transformed.csv')\n",
    "\n",
    "# avoid this ugly slicing by using a two-dim dataset\n",
    "X = data_transformed.iloc[:,:-1]\n",
    "y = data_transformed.iloc[:,:]['Class']\n",
    "\n",
    "\n",
    "# using 75% of the data for training and 25% for testing (with stratification for imbalanced class)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, stratify= y, random_state = 123)\n",
    "\n",
    "#Standardize features by removing the mean and scaling to unit variance\n",
    "sc = StandardScaler()\n",
    "scaled_X_train = sc.fit_transform(X_train)\n",
    "scaled_X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE:                                                                                                                 \n",
      "0.9994041708043694                                                                                                     \n",
      "SCORE:                                                                                                                 \n",
      "0.9993190523478508                                                                                                     \n",
      "SCORE:                                                                                                                 \n",
      "0.9995602213079869                                                                                                     \n",
      "SCORE:                                                                                                                 \n",
      "0.9990636969782948                                                                                                     \n",
      "SCORE:                                                                                                                 \n",
      "0.99936161157611                                                                                                       \n",
      "SCORE:                                                                                                                 \n",
      "0.9995176620797276                                                                                                     \n",
      "SCORE:                                                                                                                 \n",
      "0.9992339338913321                                                                                                     \n",
      "SCORE:                                                                                                                 \n",
      "0.9989643921123563                                                                                                     \n",
      "SCORE:                                                                                                                 \n",
      "0.9993757979855299                                                                                                     \n",
      "SCORE:                                                                                                                 \n",
      "0.9988367144275784                                                                                                     \n",
      "100%|███████████████████████████████████████████| 10/10 [2:57:42<00:00, 1066.29s/trial, best loss: -0.9995602213079869]\n",
      "{'C': 20.22424088022293, 'gamma': 0.005597946604318103, 'kernel': 1}\n"
     ]
    }
   ],
   "source": [
    "# Defining the space for hyperparameter tuning\n",
    "\n",
    "'''\n",
    "hp.choice(label, options) — Returns one of the options, which should be a list or tuple.\n",
    "hp.loguniform(label, low, high) — Returns a value drawn according to exp(uniform(low, high)) \n",
    "so that the logarithm of the return value is uniformly distributed.When optimizing, \n",
    "this variable is constrained to the interval [exp(low), exp(high)].\n",
    "\n",
    "'''\n",
    "\n",
    "space =  {\n",
    "   'C':hp.loguniform(\"C\", np.log(1), np.log(100)),\n",
    "   'kernel':hp.choice('kernel',['rbf', 'poly']),\n",
    "   'gamma': hp.loguniform(\"gamma\", np.log(0.001), np.log(0.1)),    \n",
    "}\n",
    "\n",
    "#SVM Parameters\n",
    "\n",
    "'''\n",
    "C — Regularization parameter. The strength of the regularization is inversely proportional to C.\n",
    "kernel — Specifies the kernel type to be used in the algorithm.\n",
    "gamma — Kernel coefficient for ‘rbf’, ‘poly’.\n",
    "\n",
    "'''\n",
    "\n",
    "# Define the function to minimize (SVM Model)\n",
    "def objective(space):\n",
    "    clf = svm.SVC( C = space['C'], kernel= space['kernel'], gamma = space['gamma']) \n",
    "    evaluation = [(scaled_X_train, y_train), (scaled_X_test, y_test)]\n",
    "    \n",
    "# Fit the model on training set    \n",
    "    clf.fit(scaled_X_train, y_train)\n",
    "# Make a prediction \n",
    "    pred = clf.predict(scaled_X_test)\n",
    "# Calculate our Metric - accuracy\n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    print (\"SCORE:\", accuracy )\n",
    "    \n",
    "# Because fmin() tries to minimize the objective, this function must return the negative accuracy. \n",
    "    return {'loss': -accuracy, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "# Initialize trials object.\n",
    "trials = Trials()\n",
    "#using seed to get repeatable results.\n",
    "seed = 123\n",
    "# run the hyper paramter tuning.\n",
    "best = fmin(fn=objective,   \n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=10,\n",
    "            trials=trials,\n",
    "           rstate= np.random.RandomState(seed))\n",
    "\n",
    "print (best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, ‘best’ gives you the optimal parameters that best fit model and better loss function value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze results by using trials object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'loss': -0.9994041708043694, 'status': 'ok'},\n",
       " {'loss': -0.9993190523478508, 'status': 'ok'},\n",
       " {'loss': -0.9995602213079869, 'status': 'ok'},\n",
       " {'loss': -0.9990636969782948, 'status': 'ok'},\n",
       " {'loss': -0.99936161157611, 'status': 'ok'},\n",
       " {'loss': -0.9995176620797276, 'status': 'ok'},\n",
       " {'loss': -0.9992339338913321, 'status': 'ok'},\n",
       " {'loss': -0.9989643921123563, 'status': 'ok'},\n",
       " {'loss': -0.9993757979855299, 'status': 'ok'},\n",
       " {'loss': -0.9988367144275784, 'status': 'ok'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials.results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‘trials’, it is an object that contains or stores all the statistical and diagnostic information such as hyperparameter, loss-functions for each set of parameters that the model has been trained. ‘fmin’, it is an optimization function that minimizes the loss function and takes in 4 inputs. Algorithm used is ‘tpe.suggest’ , other algorithm that can be used is ‘tpe.rand.suggest’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import space_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 20.22424088022293, 'gamma': 0.005597946604318103, 'kernel': 'poly'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using space_eval for finding best parameters\n",
    "space_eval(space, best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion_matrix:\n",
      "[[70364     3]\n",
      " [   28    95]]\n",
      "roc_auc_score:\n",
      "0.8861575449781812\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     70367\n",
      "           1       0.97      0.77      0.86       123\n",
      "\n",
      "    accuracy                           1.00     70490\n",
      "   macro avg       0.98      0.89      0.93     70490\n",
      "weighted avg       1.00      1.00      1.00     70490\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model SVM with best parameters \n",
    "clf = svm.SVC( C = 20.22424088022293  , gamma = 0.005597946604318103  , kernel= 'poly') \n",
    "clf.fit(scaled_X_train, y_train)\n",
    "pred = clf.predict(scaled_X_test)\n",
    "\n",
    "#Compute confusion matrix to evaluate the accuracy of a classification.\n",
    "print(\"confusion_matrix:\")\n",
    "print(confusion_matrix(y_test, pred))\n",
    "\n",
    "#Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores.\n",
    "print(\"roc_auc_score:\" )\n",
    "print( roc_auc_score(y_test, pred))\n",
    "\n",
    "#Build a text report showing the main classification metrics.\n",
    "print(\"classification_report:\")\n",
    "print( classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ". For class==0 we got all values precision, recall, f1-score as 1.00. For Class==1 precision is 0.97, recall is 0.77, f1-score is 0.86 which is pretty good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task is done in a team of 2 students. The given dataset was analyzed and modelled using SVM Model. Hyperparameters were tuned using hyperopt. Hyperparameter tuning is an important step in building a learning algorithm model. Best parameters for SVM model are 'C': 20.22424088022293, 'gamma': 0.005597946604318103, 'kernel': 'poly'. Modelled SVM with these hyperparameters. For class==0 we got all values precision, recall, f1-score as 1.00. For Class==1 precision is 0.97, recall is 0.77, f1-score is 0.86 which is pretty good. Recall can be thought of as a measure of classifier completeness."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
